# ğŸŒ¤ï¸ Weather Data Pipeline Dashboard

![Pipeline](assets/weather-pipeline.png)

This project is a containerized data pipeline for collecting, transforming, storing, and visualizing weather data. It uses Airflow for orchestration, DBT for transformation, PostgreSQL for storage, and Superset for visualization. The weather data is collected from [weatherstack](https://weatherstack.com/).

---

## ğŸ”§ Tech Stack

- **Airflow** â€“ Workflow orchestration
- **DBT** â€“ Data transformation
- **PostgreSQL** â€“ Data storage
- **Apache Superset** â€“ Dashboard & visualization
- **Docker** â€“ Containerization

---

## ğŸ“ Project Structure

```bash
.
â”œâ”€â”€ .venv/                  # Python virtual environment (ignored)
â”œâ”€â”€ airflow/                # Airflow dags and configs
â”‚   â””â”€â”€ dags/
â”‚       â””â”€â”€ orchestrator.py
â”œâ”€â”€ api-request/            # Scripts to fetch and insert weather data
â”‚   â”œâ”€â”€ api_request.py
â”‚   â””â”€â”€ insert_records.py
â”œâ”€â”€ dbt/                    # DBT project and logs
â”‚   â””â”€â”€ my_project/
â”‚       â””â”€â”€ models/
â”œâ”€â”€ docker/                 # Docker configuration files
â”‚   â”œâ”€â”€ .env                # Environment variables (not committed)
â”‚   â”œâ”€â”€ docker-init.sh
â”‚   â”œâ”€â”€ docker-bootstrap.sh
â”‚   â””â”€â”€ superset_config.py
â”œâ”€â”€ postgres/               # Database init scripts and volume mount
â”‚   â”œâ”€â”€ airflow_init.sql
â”‚   â””â”€â”€ superset_init.sql
â”œâ”€â”€ docker-compose.yaml     # Compose setup for the entire project

```

## ğŸš€ Getting Started

### 1. Clone the repository

```bash
git clone https://github.com/yourusername/weather-data.git 
cd weather-data
```

### 2. Start the services

```bash
docker-compose up --build
```

This will spin up:
-   Airflow Webserver & Scheduler
-   PostgreSQL
-   Superset
-   Your custom data fetch scripts (via Airflow DAGs)

---

## ğŸŒ€ Workflow Overview

1.  **Fetch weather data** via API (`api_request.py`)
2.  **Insert raw data** into PostgreSQL (`insert_records.py`)
3.  **Orchestrate the process** using Airflow (`orchestrator.py`)
4.  **Transform the data** using DBT
5.  **Visualize** in Superset dashboard

---

## ğŸ—‚ï¸ Airflow UI

You can monitor and trigger DAGs at:

```bash
http://localhost:8080
```

---

## ğŸ“Š Superset Dashboard

Once everything is up, you can access Superset at:

```bash
http://localhost:8088
```

Default login:
-   **Username**: admin
-   **Password**: admin

### Preview

![Dashboard](assets/weather-dashboard.png)

